Abstract

Scheme's macros are powerful tools that are easily capable of doing language
transformations within the compiler.  We use the ability to break down a parser
generator into its component languages.  As a result, we have a tool that is
much more modular, extensible, and user-friendly than the monolithic versions.

Introduction

Parser generators are standard tools that have been around for many years.  The
most famous of these tools are YACC [4] and its GNU equivalent, Bison [3].
Parser generators for Scheme are not new either [1][5][7].  However, all of
these tools suffer from the problem of being monolithic.  That is, they all have
many different parts and package many different design decisions in a single
tool.  Among these are:

- The language level.  This is usually LALR(1) although Essense [1] gives the
  user a choice between SLR(k) and full LR(k).
- The target language.  This decision is particularly evident in the fact that
  there are parser-generators for C, different generators for Java, and still
  more for Scheme, ML, Python, etc.
- Runtime issues.  Should the generated state machine be interpreted or compiled
  directly into the target language?  Are any optimizations made?
- The semantic action interface.  Most tools follow YACC's example [4] and use
  "$1", "$2", ... "$n" to represent the values of tokens although some [5] have
  taken a different approach.
- The lexical analyzer interface.  Most implementations have a fixed interface
  which means that the lexer has to be replaced if a different parser tool is
  chosen.

These design decisions may be OK for most people but not everyone.
Unfortunately, the size of these tools precludes users from writing their own
versions.  In many cases, the user only needs to change one aspect of the tool.
However, none of these tools offer a good way to do this and users are stuck
with either using what they have or nothing at all.

Taking Apart the Problem

An extremely elegant solution becomes visible when the problem is viewed in a
larger context.  The parser-generator defines its own "little language" to
handle a task it does very well.  However, it is rarely the only language used
to solve a problem.  There is usually another "little language" called the
"lexical analyzer" that comes before it and a more powerful, general-purpose
language usually comes after it and glues the components together.  There is
very little reason why we cannot take this further and break the parser down
into other languages.

The parser-generator is essentially a compiler.  It translates a Context-Free
Grammar (CFG) into a Push-Down Automaton (PDA).  The PDA acts as an intermediate
between the token stream (input language) and the semantic actions.  The
semantic actions are yet another language which is embedded in the CFG but
passes through the parser-generator either mostly or completely unmodified.  The
semantic actions, in their turn, translate the token stream into the output
language.

All told, there are five different languages involved:
1) The token alphabet
2) The CFG language
3) The PDA language
4) The semantic action language
5) The target language

Now the problem becomes one of how compose all these languages.  Shivers, in his
paper on "little languages" [6], has a ready solution.  He shows how the
Scheme's macro system can be used to create entirely new languages that can be
embedded directly in Scheme.  Furthermore, users can escape out of these macros
back into Scheme.  This allows users to embed Scheme code or still other "little
languages" inside the new language.  In the case of a parser-generator where
languages are embedded between, inside of, and on top of other languages, this
is a powerful tool.

With this in mind, we defined a CFG and PDA language and implemented macros to
handle the common cases.  In addition to this, our PDA compiler has a very
general notion of a token stream.  It can handle many different ways of
expressing tokens.  This is opposed to the normal interface to the lexical
analyzer which demands that the lexer's language be restricted to a very strict
alphabet.

The CFG Language

Grammar

cfg        ::= (cfg-clause ...)
cfg-clause ::= (COMMENT whatever ...)
             | (TOKENS token-decl ...)
             | (NO-SHIFT ident ...)
             | (END-OF-PARSE ident ...)
             | (NON-TERM ident nt-clause ...)
token-decl ::= ident
             | (NON ident ...)
             | (RIGHT ident ...)
             | (LEFT ident ...)
             | (EOS ident)
             | (ERROR ident)
nt-clause  ::= (COMMENT whatever ...)
             | (=> [ident] (ident ...) action)
action     ::= whatever

Tokens

The names of the different token classes must be declared in TOKENS.  The
following is an example declaration taken from the parser for a simple
calculator:

(TOKENS NUM L-PAREN R-PAREN SEMICOLON
	(RIGHT UMINUS)
	(LEFT  TIMES DIVIDE)
	(LEFT  PLUS MINUS)
	(ERROR *ERROR*)
	(EOS   *EOF*))

The token class names must be symbols and disjoint from on another.  LEFT,
RIGHT, and NON are used to declare precedence levels and associativity.  Tokens
that are not declared inside a precedence level are considered part of the
lowest precedence class and any shift/reduce conflict involving those tokens
will generate a warning from the CFG compiler.

ERROR is used to declare a token to use as the error symbol.  With the exception
of naming this token as the error token in the output, the CFG compiler treats
this token just like any other.  It is the PDA handles this token specially.
See below for a full discussion of this issue.

EOS ("End Of Stream") is simply syntactic sugar for a END-OF-PARSE, NO-SHIFT,
and zero precedence token.

Partial Parsing

Our language includes the NO-SHIFT and END-OF-PARSE declarations found in
ML-Yacc [8].  These declarations do not appear in YACC [4] because of its
implicit assumption that the parser should continue parsing until it reaches the
End-of-File.  There are many cases where this is not what is needed.  Consider
the built-in function READ.  It returns only a single value and not the entire
input stream.  The following is an example grammar for an simple version of
READ.

((TOKENS       *EOF* SYMBOL NUMBER STRING L-PAREN R-PAREN)
 (END-OF-PARSE *EOF* SYMBOL NUMBER STRING L-PAREN)
 (NO-SHIFT     *EOF*)

 (NON-TERM item
           (=> (SYMBOL)                    SYMBOL)
           (=> (NUMBER)                    NUMBER)
           (=> (STRING)                    STRING)
           (=> (L-PAREN item-list R-PAREN) (reverse item-list)))
 (NON-TERM item-list
           (=> ()                          '())
           (=> (item-list item)            (cons item item-list))))

END-OF-PARSE means that the tokens so named can occur after the start symbol and
are valid lookaheads to guard the ACCEPT actions.  In YACC, the parser could
only accept when it reached the End-of-File.  By allowing other tokens to appear
at the end of a parse, we can parse only part of a stream.

NO-SHIFT means that the token should never be read past when encountered on the
input stream.  This implies that it should never be shifted by the PDA but it
also has an effect on error correction.  When in error correction mode, the PDA
will try to throw away tokens until it can re-sync with the grammar.  It cannot
throw away NO-SHIFT tokens because that would be equivalent to reading past
them.  The End-of-File is a NO-SHIFT token because once the parser has reached
the End-of-File, it is not going to get anything else.

Non-Terminals

The NON-TERM declaration declares a non-terminal symbol and all the rules
associated with it.  The names of the non-terminals must be distinct from the
names of the token classes.  The first NON-TERM symbol is considered the start
symbol.

As an example, consider the following grammar for parsing a list of
semicolon-delimited, infix math expression.

((COMMENT "A simple, infix calculator")
 (TOKENS NUM L-PAREN R-PAREN SEMICOLON
	 (RIGHT UMINUS)
	 (LEFT  TIMES DIVIDE)
	 (LEFT  PLUS MINUS)
	 (EOS   *EOF*))

 (NON-TERM program
	   (=>        (smt-list)		(reverse smt-list)))
 (NON-TERM smt-list
	   (=>        (exp)			(list exp))
	   (=>	      (smt-list SEMICOLON exp)	(cons exp smt-list)))
 (NON-TERM exp
	   (=>        (NUM)			NUM)
	   (COMMENT Handle a special high-precedence case of MINUS)
	   (=> UMINUS (MINUS exp)		(- exp))
	   (=> 	      (exp PLUS exp)		(+ exp-1 exp-2))
	   (=>        (exp MINUS exp)		(- exp-1 exp-2))
	   (=>        (exp TIMES exp)		(* exp-1 exp-2))
	   (=>        (exp DIVIDE exp)		(quotient exp-2 exp-2))
	   (=>        (L-PAREN exp R-PAREN)	exp))

Our CFG grammar is very similar to that used by other tools.  The
right-hand-sides for each non-terminal are declared together, defining a rule.
An optional precedence declaration may precede each rule.  If this is absent,
the precedence for the rule is taken to be the precedence of the last terminal
symbol used in the rule (if any).

The semantic action for each rule follows the right-hand-side.  In the above
examples, they are all Scheme code but the CFG compiler does not care.  It
passes them through to the output unmodified so.  The name binding in the
semantic actions is done in a separate step.  See *Semantic Actions*, below, for
a detailed description.

The Parser-Generator Macro

Translating from a CFG to a PDA is easy.  A simple macro called CFG->PDA is
provided.  The code for this macro is a heavily adapted version of Boucher's
parser [1] and implements the algorithm for constructing a LALR(1) parser
described in [2].  We went with this solution because the algorithm is very
efficient and yet handles most grammars.

The PDA Language

The Automaton

A Push-Down Automaton is the abstract machine on which a LR(k) grammar executes.
It consists of an action table and two stacks, a state stack and a token stack.
The action table is indexed by state and lookahead symbols.  The values in the
action table can be one of the following:

- (SHIFT state)
    Push STATE onto the state stack and the token onto the token stack.
- (REDUCE non-terminal arity action)
    Pop ARITY tokens from each state.  Use the popped tokens from the token-stack
  as arguments to ACTION.  Push the result of calling ACTION onto the value stack.
  Then, use NON-TERMINAL to index into the action-table and follow the resulting
  GOTO action.
- (GOTO state)
    These can only appear as actions for non-terminals and non-terminals can have
  no other action.  They are used to mark which state to goto after a reduction
  has occurred.
- (ACCEPT)
    Stop parsing and return the value on the value stack (there should be only
  one).
- If an index of the action table does not have an action associated with it, the
  action is considered to be ERROR.

 SHIFT (pushes states and value onto the value stack), REDUCE (pops them off and
 runs semantic actions), or ACCEPT (which terminates the loop).  There is a
 special section of the action table that is indexed by non-terminal symbols.
 The only actions that are allow in this section are GOTO.  These name states to
 goto after a reduction has occurred.  Any element of the action table that does
 not have an action associated with it is considered to be an error action.

The Grammar

pda           ::= (pda-clause ...)
pda-clause    ::= (COMMENT whatever ...)
                | (TOKENS ident ...)
                | (ERROR ident)
                | (NO-SHIFT ident ...)
                | (STATE state-name action-clause ...)
                | (RULE rule-name non-term arity action)

The PDA language, like the CFG language, consists of a series of
keyword-delimited lists.  COMMENTs are ignored.  TOKENS is used to specify the
alphabet of tokens.  ERROR denotes the error token.  NO-SHIFT means the same as
in the CFG language.

pda-clause    ::= (STATE state-name action-clause ...)
action-clause ::= (COMMENT whatever ...)
                | (SHIFT lookahead state-name)
                | (REDUCE lookahead rule-name)
                | (ACCEPT lookahead)
                | (GOTO non-term state-name)
lookahead     ::= (ident ...)
state-name    ::= ident
rule-name     ::= ident
token         ::= ident
non-term      ::= ident

The action table is defined a row at a time using the STATE clause.  SHIFT,
REDUCE, ACCEPT, and GOTO are the actions that take place in the given state for
the given lookahead.  STATE-NAME and RULE-NAME are the names of the states to
shift into and rules to reduce by, respectively.

Lookaheads are lists of tokens.  These lists can be of any length although or
CFG compiler will only generate zero or one element lists.  An empty list
denotes a default action--an action that takes place if no other action-clause
matches.  If there is no default action and no action clause matches the input,
then the action is assumed to be an error.

The same thing extends to LR(k) grammars with k > 1.

(STATE s22 (SHIFT  (ID ASSIGN) s23)
           (REDUCE (ID)        r7)
           (GOTO   assign-smt  s34))

This state will shift when an ID followed by an ASSIGN appears on the input
stream, REDUCE when ID is followed by anything else, and go into error recovery
when ID is not the next thing on the input stream.

GOTO is similar to the other actions except it switches based on a single
non-terminal.  In the example above, if a future state accessed by the SHIFT to
s23 reduced by a rule with 'assign-smt' on the left-hand-side, the PDA would
shift into state s34 after the reduction took place.

pda-clause    ::= (RULE rule-name non-term arity action)
arity         ::= NUMBER
                | (ident ...)

When a reduction occurs, the PDA needs to know how many states to pop, what
non-term to use for the following goto action, and what semantic action to run.
Since it is quite common for a CFG rule to be used in more than one REDUCE
action in the PDA, the rules are named by this clause and then referred to by
name in the PDA.

ARITY is syntactic sugar to allow a rule-clause to either look very similar to a
CFG rule or only contain the information necessary.  The PDA needs to know the
number of states to pop and if ARITY is a list, then that number is taken to be
the length of the list.

(RULE r11 assign-smt (ID ASSIGN exp) (list "assign" ID exp))
(RULE r11 assign-smt 3               (list "assign" ID exp))

The previous two rules mean the same thing as far as the PDA compiler is
concerned.

The Language as Documentation and Specification

Many parser-generator tools will print out the generated PDA as a debugging
tool.  Our parser-generator is unique in that it generates an executable
specification.  This specification can also serve as a debugging tool.  Consider
the following excerpt from a grammar that contains the dangling-else problem:

(STATE s8
        (COMMENT exp "=>" "." EXP)
        (COMMENT smt "=>" IF exp THEN smt-list "." ELSE smt-list)
        (COMMENT smt "=>" "." IF exp THEN smt-list ELSE smt-list)
        (COMMENT smt "=>" IF exp THEN smt-list ".")
        (COMMENT smt "=>" "." IF exp THEN smt-list)
        (COMMENT smt "=>" "." exp)
        (COMMENT smt-list "=>" smt-list "." smt)

        (COMMENT (REDUCE (IF) r5))
        (SHIFT (IF) s4)
        (COMMENT (REDUCE (ELSE) r5))
        (SHIFT (ELSE) s10)
        (COMMENT (REDUCE (EXP) r5))
        (SHIFT (EXP) s5)
        (REDUCE (*EOF*) r5)

        (GOTO smt s9)
        (GOTO exp s3))

Here we see that the list of items is kept with each state using the COMMENT
declarations.  Furthermore, the conflicting actions are kept together with all
but one of them COMMENTed out.  The user can easily see from the specification
what is going on in a particular state and where the problems are.

The user also has an option that no other tool permits: he can edit the
generated PDA.  This could be used to do manual conflict resolution or to
increase the power of the PDA.  For example, it might be possible to add only
one or two states to create a PDA for an non-LALR(1) grammar without resorting
to more powerful tools.  In any case, the user is not restricted to just editing
the CFG representation.

The PDA Compiler

Our PDA compiler will take a LR(1) PDA with Scheme semantic actions and
translate it into a Scheme function.  Again, we chose this particular subset
because it is the most common case.  The compiler itself is a macro and looks
like:

  (parse/pda get-token drop-token token-case [parse-error] pda)

The macro expands into a function that takes a token-stream argument and returns
a pair containing the semantic value of the start symbol and the leftover
token-stream.  GET-TOKEN, DROP-TOKEN, and TOKEN-CASE define the interface with
the lexical analyzer.  PARSE-ERROR is an option function that will be called
when an error occurs.  It has the same semantics as in Bison [3].

There are two aspects to our lexer interface that are different from most other
implementations: functional streams and a separation of the token class from the
token value.  The first difference was to facilitate some of our other design
decisions while the second difference allows us to generalize the language for
the lexical analyzer.

The functional stream interface is encoded using GET-TOKEN and DROP-TOKEN.
GET-TOKEN is a function which takes a stream and a lookahead number and returns
the correct token from the stream.  (The number will always be 1 for LR(1)
grammars.)  GET-TOKEN does not eat a token.  DROP-TOKEN does that.  DROP-TOKEN
takes a stream as an argument and returns another stream with the first value
removed.  The best way to think of these two is like CAR and CDR.  If the stream
is a list and only a single element of lookahead is used, the only difference
between GET-TOKEN and CAR is that GET-TOKEN will return the End-of-Stream token
when the list is null rather than generating an error.

We decided to use functional streams in order to facilitate using symbols other
than the End-of-Stream symbol to mark the end of the parse.  Such symbols have
to show up on the token stream but because they are not shifted by the parser,
they should not be taken off the stream.  Thus, the mechanism for lookaheads has
to be separated from the mechanism for taking a token off the stream.  This
could be provided by a "peek" function of some type but functional streams also
allow this and are a better idea anyway.

TOKEN-CASE is the means to generalize the token-value space.  It allows our
compiler to handle tokens streams that span the complexity gamut from simple
character streams to representations such as NUM, STRING, and ID encoded as
numbers, strings, and symbols to records that contain everything including
position information.  TOKEN-CASE acts as a translator between whatever the
token values are the token classes declared at the top of the PDA.

The syntax of TOKEN-CASE is exactly the same as the built-in keyword CASE.  It
takes a token (returned by GET-TOKEN) as the switch and the options are the
token classes declared in the PDA and CFG.  TOKEN-CASE is a keyword or macro
that generates a decision tree to determine what class a given token is.  This
decision tree could be something as dumb as a big CODE statement or, preferably,
something more efficient like a binary-search IF-nesting or a jump table.

Semantic Actions

The semantic actions in all the examples so far have been Scheme code.  This is
not required by the CFG->PDA compiler but the PDA compiler generates Scheme code
and assumes that the semantic actions are the same.  To be more specific, the
PDA compiler assumes that semantic actions are Scheme functions with the same
number of parameters as the rule the action is for.  This would get cumbersome
to write by hand so there is a preprocessor for the the CFG language that
handles the conversion.  It translates this:

(NON-TERM if-smt
	  (=> (IF exp THEN exp ELSE exp) (if exp-1 exp-2 exp-3)))

into this:

(NON-TERM if-smt
	  (=> (IF exp THEN exp ELSE exp)
              (lambda (IF exp-1 THEN exp-2 ELSE exp-3)
                (if exp-1 exp-2 exp-3))))

The appending of "-1", "-2", etc. is the default way of resolving the conflict
when more than one symbol with the same name is used.  This is not the only way
of resolving the conflict.  The user can name each symbol himself.

(NON-TERM if-smt
	  (=> (IF (test-exp exp) THEN (then-exp exp) ELSE (else-exp exp))
	      (if test-exp then-exp else-exp)))

Benefits of the Little Languages Approach

The little languages approach is an extremely powerful tool for breaking down
complex problems.  Scheme allows the user to "mix and match" these languages and
tools in whatever way is needed.  Our parser-generator could be taken out and
replaced with a more general LR generator.  Alternatively, it could be replaced
with a faster SLR generator.  On the other hand, our PDA compiler could be
replaced with an interpreter or an optimizer could be inserted into the mix.
The modularization potential of this is enormous.

As an example, consider what it would take to write YACC given the tools
described in this paper.  The only thing that would have to be done is to write
the PDA compiler.  The semantic actions could be strings that look like C code
(this is how YACC does it).  The tokens can be #defined-ed just like YACC does.
The parser-generator algorithm can be used as is.  The only thing not written is
the PDA compiler.  The PDA is nothing more than a state machine and its
semantics are well understood.  Writing this PDA would likely get very boring.

However, for all this power and abstraction, there is no burden placed on the
end user.  We defined a simple, all-encompassing macro that combines the
standard tools.  It is called PARSE/CFG and simply takes the CFG and lexer
interface, compiles the semantic actions, compiles the CFG, and then generates
the PDA function.  The user does not have to worry about any of the intermediate
steps.  Thus, language hackers have enormous power at their fingertips while the
common user can just write out a grammar with very little trouble.

Future Work

The most obvious way in which our tool could be improved is that additional
modules could be added to it.  A parser generator that compiled LR(k) grammars
could be useful.  Also, it might be interesting to look into the kinds of
optimizations that can happen on a PDA.

Another area to look into is the creation of a lexical analyzer tool would round
out the capabilities provided by our tool.  Our tool has a very general lexer
interface but doing IO by hand is still cumbersome.  A tool to handle this would
be useful and potentiall interesting as there are probably languages involved
with it as well (regular expressions, many different types of input, etc.)

Finally, error handling needs a second look.  Our solution uses YACC-style error
correction because it was the easiest to implement.  There are other methods,
such as that used by ML-Yacc [8], and it would be nice for the error protocol to
be orthogonal to the other languages involved.  We thought about this but could
not find a good way to handle it.  Error handling is handled when the PDA
executes but it must be defined back at the CFG level.  Any error protocol that
is chosen has to survive two language translations.

References

[1] Boucher, D.  An Efficient and Portable Scheme LALR(1) Parser Generator
    [online].  Available from:
    http://www.iro.umontreal.ca/~boucherd/Lalr/documentation/lalr.html
[2] DeRemer, F. and T. Pennello.  Efficient Computation of LALR(1) Look-Ahead
    Sets.  In ACM Transactions on Programming Languages and Systems Vol. 4,
    No. 4, October 1982.
[3] Free Software Foundation.  Bison [online].  Available from:
    http://www.gnu.org/software/bison/bison.html
[4] Johnson, S.C.  YACC--Yet another compiler compiler.  Tech. Rep. CSTR 32,
    Bell Labs., Murray Hill, N.J., 1974.
[5] Serrano, M.  Bigloo, A "practical Scheme compiler," User Manual for version
    2.6b, November 2003 [online].  Available from:
    http://www-sop.inria.fr/mimosa/fp/Bigloo/doc/bigloo.pdf
[6] Shivers, O.  A universal scripting framework, or Lambda: the ultimate
    "little language."  In Concurrency and Parallelism, Programming, Networking,
    and Security, Lecture Notes in Computer Science. pages 254-265, Editors
    Joxan Jaffar and Roland H.-C.-Yap, 1996, Springer.
[7] Sperber, M. and P. Thiemann.  Essence--An LR Parser Generator for Scheme
    [online].  Available from:
    http://www.informatik.uni-freiburg.de/proglang/software/essence/
[8] Tarditi, D. and A. Appel.  ML-Yacc User's Manual [online].  Available from:
    http://www.smlnj.org/doc/ML-Yacc/
